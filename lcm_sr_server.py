"""
lcm_sr_server.py â€” RKNN LCM Stable Diffusion FastAPI server (queued, multi-worker safe)

Key goals:
- One pipeline per worker thread (no shared RKNN objects across threads)
- Determin guarantee: per-request seed -> np.RandomState
- Deterministic input ordering handled in RKNN2Model (recommended)
- Explicit data_format per model (UNet + VAE commonly NHWC on RKNN)
- Queue backpressure (429 on overflow)
- Clean startup/shutdown (FastAPI lifespan)
- Returns image bytes + X-Seed header
- Super-resolution:
  - As postprocess on /generate (req.superres=true)
  - As standalone ingest endpoint /superres (multipart upload)
  - Magnitude (1/2/3) controls number of SR passes; defaults to 2

Env:
  MODEL_ROOT=/models/lcm_rknn
  PORT=4200
  NUM_WORKERS=1..3
  QUEUE_MAX=64
  DEFAULT_SIZE=512x512
  DEFAULT_STEPS=4
  DEFAULT_GUIDANCE=1.0
  DEFAULT_TIMEOUT=120

  SR_ENABLED=true|false
  SR_MODEL_PATH=/models/lcm_rknn/super-resolution-10.rknn
  SR_INPUT_SIZE=224
  SR_OUTPUT_SIZE=672
  SR_NUM_WORKERS=1..N
  SR_QUEUE_MAX=32
  SR_REQUEST_TIMEOUT=120
  SR_MAX_PIXELS=24000000
"""

import io
import os
import json
import time
import queue
import threading
from dataclasses import dataclass
from concurrent.futures import Future
from typing import Optional, List, Dict, Tuple
from contextlib import asynccontextmanager

import numpy as np
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.responses import Response
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field

from diffusers import LCMScheduler
from transformers import CLIPTokenizer
from PIL import Image

from rknnlite.api import RKNNLite
from rknnlcm import RKNN2Model, RKNN2LatentConsistencyPipeline

# lcm_sr_server.py (add near imports)
from compat_endpoints import CompatEndpoints

# -----------------------------
# Request schema (HTTP)
# -----------------------------
class GenerateRequest(BaseModel):
    prompt: str
    size: str = Field(default=os.environ.get("DEFAULT_SIZE", "512x512"), pattern=r"^\d+x\d+$")
    num_inference_steps: int = Field(default=int(os.environ.get("DEFAULT_STEPS", "4")), ge=1, le=50)
    guidance_scale: float = Field(default=float(os.environ.get("DEFAULT_GUIDANCE", "1.0")), ge=0.0, le=20.0)
    seed: Optional[int] = Field(default=None, ge=0, le=2**31 - 1)

    # ---- postprocess ----
    superres: bool = Field(default=False, description="If true, run RKNN super-resolution as a postprocess step.")
    superres_format: str = Field(default="png", pattern=r"^(png|jpeg)$")
    superres_quality: int = Field(default=92, ge=1, le=100, description="JPEG quality if superres_format=jpeg.")
    superres_magnitude: int = Field(
        default=2,
        ge=1,
        le=3,
        description="SR magnitude (1..3). Interpreted as number of SR passes. Default=2.",
    )


@dataclass(frozen=True)
class ModelPaths:
    root: str

    @property
    def scheduler_config(self) -> str:
        return os.path.join(self.root, "scheduler", "scheduler_config.json")

    @property
    def text_encoder(self) -> str:
        return os.path.join(self.root, "text_encoder")

    @property
    def unet(self) -> str:
        return os.path.join(self.root, "unet")

    @property
    def vae_decoder(self) -> str:
        return os.path.join(self.root, "vae_decoder")


@dataclass
class Job:
    req: GenerateRequest
    fut: Future
    submitted_at: float


# -----------------------------
# RKNN multi-context configuration
# -----------------------------
def build_rknn_context_cfgs_for_rk3588(num_workers: int) -> List[dict]:
    core_masks = ["NPU_CORE_0", "NPU_CORE_1", "NPU_CORE_2"]
    cfgs = []
    for i in range(num_workers):
        cfgs.append(
            {
                "multi_context": True,
                "core_mask": core_masks[i % len(core_masks)],
                "context_name": f"w{i}",
                "worker_id": i,
            }
        )
    return cfgs


def parse_size(size_str: str) -> Tuple[int, int]:
    w_str, h_str = size_str.lower().split("x")
    w, h = int(w_str), int(h_str)
    if w <= 0 or h <= 0:
        raise ValueError("size must be positive")
    return w, h


def gen_seed_8_digits() -> int:
    return int(np.random.randint(0, 100_000_000))


# -----------------------------
# Pipeline Worker
# -----------------------------
class PipelineWorker:
    """
    Owns ONE pipeline instance. Execute jobs sequentially on this worker.
    """

    def __init__(
        self,
        worker_id: int,
        paths: ModelPaths,
        scheduler_config: Dict,
        tokenizer: CLIPTokenizer,
        rknn_context_cfg: Optional[dict] = None,
        use_rknn_context_cfgs: bool = True,
    ):
        self.worker_id = worker_id
        self.paths = paths
        self.scheduler_config = scheduler_config
        self.tokenizer = tokenizer
        self.rknn_context_cfg = rknn_context_cfg or {}
        self.use_rknn_context_cfgs = use_rknn_context_cfgs

        self.pipe = None
        self._init_pipeline()

    def _mk_model(self, model_path: str, *, data_format: str) -> RKNN2Model:
        if self.use_rknn_context_cfgs:
            return RKNN2Model(model_path, data_format=data_format, **self.rknn_context_cfg)
        return RKNN2Model(model_path, data_format=data_format)

    def _init_pipeline(self):
        scheduler = LCMScheduler.from_config(self.scheduler_config)
        self.pipe = RKNN2LatentConsistencyPipeline(
            text_encoder=self._mk_model(self.paths.text_encoder, data_format="nchw"),
            unet=self._mk_model(self.paths.unet, data_format="nhwc"),
            vae_decoder=self._mk_model(self.paths.vae_decoder, data_format="nhwc"),
            scheduler=scheduler,
            tokenizer=self.tokenizer,
        )

    def run_job(self, job: Job) -> Tuple[bytes, int]:
        width, height = parse_size(job.req.size)
        seed = job.req.seed if job.req.seed is not None else gen_seed_8_digits()
        rng = np.random.RandomState(seed)

        result = self.pipe(
            prompt=job.req.prompt,
            height=height,
            width=width,
            num_inference_steps=job.req.num_inference_steps,
            guidance_scale=job.req.guidance_scale,
            generator=rng,
        )

        pil_image = result["images"][0]
        buf = io.BytesIO()
        pil_image.save(buf, format="PNG")
        return buf.getvalue(), seed


# -----------------------------
# Singleton Service (LCM generation)
# -----------------------------
class PipelineService:
    _instance = None
    _instance_lock = threading.Lock()

    def __init__(
        self,
        paths: ModelPaths,
        num_workers: int,
        queue_max: int,
        rknn_context_cfgs: Optional[List[dict]] = None,
        use_rknn_context_cfgs: bool = True,
    ):
        self.paths = paths
        self.num_workers = max(1, int(num_workers))
        self.q: "queue.Queue[Job]" = queue.Queue(maxsize=int(queue_max))

        with open(self.paths.scheduler_config, "r") as f:
            self.scheduler_config = json.load(f)

        self.tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch16")

        if rknn_context_cfgs is None:
            rknn_context_cfgs = build_rknn_context_cfgs_for_rk3588(self.num_workers)
        if len(rknn_context_cfgs) != self.num_workers:
            raise ValueError("rknn_context_cfgs must match num_workers length")

        self.workers: List[PipelineWorker] = []
        self.threads: List[threading.Thread] = []
        self._stop = threading.Event()

        for i in range(self.num_workers):
            w = PipelineWorker(
                worker_id=i,
                paths=self.paths,
                scheduler_config=self.scheduler_config,
                tokenizer=self.tokenizer,
                rknn_context_cfg=rknn_context_cfgs[i],
                use_rknn_context_cfgs=use_rknn_context_cfgs,
            )
            self.workers.append(w)

        for i in range(self.num_workers):
            t = threading.Thread(target=self._worker_loop, args=(i,), daemon=True)
            t.start()
            self.threads.append(t)

    @classmethod
    def get_instance(
        cls,
        paths: ModelPaths,
        num_workers: int,
        queue_max: int,
        rknn_context_cfgs: Optional[List[dict]] = None,
        use_rknn_context_cfgs: bool = True,
    ) -> "PipelineService":
        with cls._instance_lock:
            if cls._instance is None:
                cls._instance = cls(
                    paths=paths,
                    num_workers=num_workers,
                    queue_max=queue_max,
                    rknn_context_cfgs=rknn_context_cfgs,
                    use_rknn_context_cfgs=use_rknn_context_cfgs,
                )
            return cls._instance

    def shutdown(self):
        self._stop.set()
        while True:
            try:
                job = self.q.get_nowait()
            except queue.Empty:
                break
            if not job.fut.done():
                job.fut.set_exception(RuntimeError("Service shutting down"))
            self.q.task_done()

    def submit(self, req: GenerateRequest, timeout_s: float = 0.25) -> Future:
        fut: Future = Future()
        job = Job(req=req, fut=fut, submitted_at=time.time())
        try:
            self.q.put(job, timeout=timeout_s)
        except queue.Full:
            fut.set_exception(RuntimeError("Queue full"))
        return fut

    def _worker_loop(self, worker_idx: int):
        worker = self.workers[worker_idx]
        while not self._stop.is_set():
            try:
                job = self.q.get(timeout=0.1)
            except queue.Empty:
                continue

            if job.fut.cancelled():
                self.q.task_done()
                continue

            try:
                png, seed = worker.run_job(job)
                if not job.fut.done():
                    job.fut.set_result((png, seed))
            except Exception as e:
                if not job.fut.done():
                    job.fut.set_exception(e)
            finally:
                self.q.task_done()


# -----------------------------
# Super-resolution worker/service
# -----------------------------
@dataclass
class SRJob:
    image_bytes: bytes
    out_format: str
    quality: int
    magnitude: int  # 1..3
    fut: Future
    submitted_at: float


class SuperResWorker:
    def __init__(self, worker_id: int, model_path: str, input_size: int, output_size: int):
        self.worker_id = worker_id
        self.model_path = model_path
        self.input_size = int(input_size)
        self.output_size = int(output_size)

        self.rknn = RKNNLite()
        self._init_runtime()

    def _init_runtime(self):
        ret = self.rknn.load_rknn(self.model_path)
        if ret != 0:
            raise RuntimeError(f"SR load_rknn failed: {ret}")
        ret = self.rknn.init_runtime()
        if ret != 0:
            raise RuntimeError(f"SR init_runtime failed: {ret}")
        print(f"[SR] worker {self.worker_id} loaded {self.model_path}")

    def close(self):
        rel = getattr(self.rknn, "release", None)
        if callable(rel):
            rel()

    def _plan_tiles(self, w: int, h: int):
        tile = self.input_size
        step = tile
        xs = list(range(0, max(1, w - tile + 1), step))
        ys = list(range(0, max(1, h - tile + 1), step))
        if not xs or xs[-1] != w - tile:
            xs.append(max(0, w - tile))
        if not ys or ys[-1] != h - tile:
            ys.append(max(0, h - tile))
        return [(x, y) for y in ys for x in xs]

    def upscale_once(self, image_bytes: bytes, out_format: str = "png", quality: int = 92) -> bytes:
        img = Image.open(io.BytesIO(image_bytes)).convert("RGB")

        if img.width * img.height > SR_MAX_PIXELS:
            raise RuntimeError(
                f"Image too large: {img.width}x{img.height} exceeds SR_MAX_PIXELS={SR_MAX_PIXELS}"
            )

        img_ycc = img.convert("YCbCr")
        img_y, img_cb, img_cr = img_ycc.split()
        img_y_np = (np.array(img_y, dtype=np.float32) / 255.0)

        in_w, in_h = img.width, img.height
        scale = self.output_size / self.input_size
        out_w = int(round(in_w * scale))
        out_h = int(round(in_h * scale))

        out_y = np.zeros((out_h, out_w), dtype=np.float32)

        tiles = self._plan_tiles(in_w, in_h)
        for (x0, y0) in tiles:
            crop = img_y_np[y0 : y0 + self.input_size, x0 : x0 + self.input_size]
            inp = crop[np.newaxis, np.newaxis, :, :].astype(np.float32)  # (1,1,H,W)

            pred = self.rknn.inference(inputs=[inp])[0]
            tile_out = pred[0, 0] if pred.ndim == 4 else pred[0][0]

            ox0 = int(round(x0 * scale))
            oy0 = int(round(y0 * scale))
            out_y[oy0 : oy0 + self.output_size, ox0 : ox0 + self.output_size] = tile_out

        out_y_u8 = np.uint8(np.clip(out_y * 255.0, 0, 255.0))
        out_img = Image.merge(
            "YCbCr",
            [
                Image.fromarray(out_y_u8, mode="L"),
                img_cb.resize((out_w, out_h), Image.BICUBIC),
                img_cr.resize((out_w, out_h), Image.BICUBIC),
            ],
        ).convert("RGB")

        buf = io.BytesIO()
        if out_format == "jpeg":
            out_img.save(buf, format="JPEG", quality=int(quality))
        else:
            out_img.save(buf, format="PNG")
        return buf.getvalue()

    def upscale_bytes(self, image_bytes: bytes, *, magnitude: int, out_format: str, quality: int) -> bytes:
        mag = int(magnitude)
        if mag < 1 or mag > 3:
            raise RuntimeError("magnitude must be 1..3")
        out = image_bytes
        for _ in range(mag):
            out = self.upscale_once(out, out_format=out_format, quality=quality)
        return out


class SuperResService:
    def __init__(self, model_path: str, num_workers: int, queue_max: int, input_size: int, output_size: int):
        self.model_path = model_path
        self.num_workers = max(1, int(num_workers))
        self.q: "queue.Queue[SRJob]" = queue.Queue(maxsize=int(queue_max))

        self.workers: List[SuperResWorker] = []
        self.threads: List[threading.Thread] = []
        self._stop = threading.Event()

        for i in range(self.num_workers):
            w = SuperResWorker(
                worker_id=i,
                model_path=self.model_path,
                input_size=input_size,
                output_size=output_size,
            )
            self.workers.append(w)

        for i in range(self.num_workers):
            t = threading.Thread(target=self._worker_loop, args=(i,), daemon=True)
            t.start()
            self.threads.append(t)

    def shutdown(self):
        self._stop.set()
        while True:
            try:
                job = self.q.get_nowait()
            except queue.Empty:
                break
            if not job.fut.done():
                job.fut.set_exception(RuntimeError("SuperResService shutting down"))
            self.q.task_done()

        for w in self.workers:
            try:
                w.close()
            except Exception:
                pass

    def submit(
        self,
        image_bytes: bytes,
        *,
        out_format: str,
        quality: int,
        magnitude: int,
        timeout_s: float = 0.25,
    ) -> Future:
        fut: Future = Future()
        job = SRJob(
            image_bytes=image_bytes,
            out_format=out_format,
            quality=int(quality),
            magnitude=int(magnitude),
            fut=fut,
            submitted_at=time.time(),
        )
        try:
            self.q.put(job, timeout=timeout_s)
        except queue.Full:
            fut.set_exception(RuntimeError("Queue full"))
        return fut

    def _worker_loop(self, worker_idx: int):
        worker = self.workers[worker_idx]
        while not self._stop.is_set():
            try:
                job = self.q.get(timeout=0.1)
            except queue.Empty:
                continue

            if job.fut.cancelled():
                self.q.task_done()
                continue

            try:
                out_bytes = worker.upscale_bytes(
                    job.image_bytes,
                    magnitude=job.magnitude,
                    out_format=job.out_format,
                    quality=job.quality,
                )
                if not job.fut.done():
                    job.fut.set_result(out_bytes)
            except Exception as e:
                if not job.fut.done():
                    job.fut.set_exception(e)
            finally:
                self.q.task_done()


# -----------------------------
# FastAPI server config
# -----------------------------
MODEL_ROOT = os.environ.get("MODEL_ROOT", "/models/lcm_rknn")
NUM_WORKERS = int(os.environ.get("NUM_WORKERS", "1"))
QUEUE_MAX = int(os.environ.get("QUEUE_MAX", "64"))
PORT = int(os.environ.get("PORT", "4200"))
REQUEST_TIMEOUT = float(os.environ.get("DEFAULT_TIMEOUT", "120"))

SR_ENABLED = os.environ.get("SR_ENABLED", "1") not in ("0", "false", "False")
SR_MODEL_PATH = os.environ.get("SR_MODEL_PATH", os.path.join(MODEL_ROOT, "super-resolution-10.rknn"))
SR_INPUT_SIZE = int(os.environ.get("SR_INPUT_SIZE", "224"))
SR_OUTPUT_SIZE = int(os.environ.get("SR_OUTPUT_SIZE", "672"))
SR_NUM_WORKERS = int(os.environ.get("SR_NUM_WORKERS", "1"))
SR_QUEUE_MAX = int(os.environ.get("SR_QUEUE_MAX", "32"))
SR_REQUEST_TIMEOUT = float(os.environ.get("SR_REQUEST_TIMEOUT", "120"))
SR_MAX_PIXELS = int(os.environ.get("SR_MAX_PIXELS", "24000000"))

USE_RKNN_CONTEXT_CFGS = os.environ.get("USE_RKNN_CONTEXT_CFGS", "1") not in ("0", "false", "False")

paths = ModelPaths(root=MODEL_ROOT)


@asynccontextmanager
async def lifespan(app: FastAPI):
    app.state.service = PipelineService.get_instance(
        paths=paths,
        num_workers=NUM_WORKERS,
        queue_max=QUEUE_MAX,
        rknn_context_cfgs=build_rknn_context_cfgs_for_rk3588(NUM_WORKERS),
        use_rknn_context_cfgs=USE_RKNN_CONTEXT_CFGS,
    )

    app.state.sr_service = None
    if SR_ENABLED:
        if not os.path.isfile(SR_MODEL_PATH):
            raise RuntimeError(f"SR model not found at SR_MODEL_PATH={SR_MODEL_PATH}")

        app.state.sr_service = SuperResService(
            model_path=SR_MODEL_PATH,
            num_workers=SR_NUM_WORKERS,
            queue_max=SR_QUEUE_MAX,
            input_size=SR_INPUT_SIZE,
            output_size=SR_OUTPUT_SIZE,
        )

    yield

    app.state.service.shutdown()
    if app.state.sr_service is not None:
        app.state.sr_service.shutdown()


app = FastAPI(lifespan=lifespan, title="LCM_Stable_Diffusion and Super_Resolution Service")


@app.post("/generate", responses={200: {"content": {"image/png": {}, "image/jpeg": {}}}})
def generate(req: GenerateRequest):
    service: PipelineService = app.state.service

    fut = service.submit(req, timeout_s=0.25)
    try:
        png_bytes, seed = fut.result(timeout=REQUEST_TIMEOUT)
    except Exception as e:
        msg = str(e)
        if "Queue full" in msg:
            raise HTTPException(status_code=429, detail="Too many requests (queue full). Try again.")
        raise HTTPException(status_code=500, detail=f"Generation failed: {msg}")

    did_superres = False
    out_bytes = png_bytes
    media_type = "image/png"

    sr_mag = int(req.superres_magnitude or 2)
    if sr_mag < 1 or sr_mag > 3:
        raise HTTPException(status_code=400, detail="superres_magnitude must be 1..3")

    if req.superres:
        sr: Optional[SuperResService] = getattr(app.state, "sr_service", None)
        if sr is None:
            raise HTTPException(status_code=503, detail="Super-resolution requested but SR service is disabled")

        sr_fut = sr.submit(
            image_bytes=png_bytes,
            out_format=req.superres_format,
            quality=req.superres_quality,
            magnitude=sr_mag,
            timeout_s=0.25,
        )
        try:
            out_bytes = sr_fut.result(timeout=SR_REQUEST_TIMEOUT)
            did_superres = True
            media_type = "image/jpeg" if req.superres_format == "jpeg" else "image/png"
        except Exception as e:
            msg = str(e)
            if "Queue full" in msg:
                raise HTTPException(status_code=429, detail="Too many requests (SR queue full). Try again.")
            raise HTTPException(status_code=500, detail=f"Super-resolution failed: {msg}")

    headers = {
        "Cache-Control": "no-store",
        "X-Seed": str(seed),
        "X-SuperRes": "1" if did_superres else "0",
    }
    if did_superres:
        headers.update(
            {
                "X-SR-Model": os.path.basename(SR_MODEL_PATH),
                "X-SR-Passes": str(sr_mag),
                "X-SR-Scale-Per-Pass": (
                    str(SR_OUTPUT_SIZE // SR_INPUT_SIZE)
                    if SR_OUTPUT_SIZE % SR_INPUT_SIZE == 0
                    else str(SR_OUTPUT_SIZE / SR_INPUT_SIZE)
                ),
                "X-SR-Format": req.superres_format,
            }
        )

    return Response(content=out_bytes, media_type=media_type, headers=headers)


@app.post("/superres", responses={200: {"content": {"image/png": {}, "image/jpeg": {}}}})
async def superres(
    file: UploadFile = File(...),
    magnitude: int = Form(2),  # default=2
    out_format: str = Form("png"),
    quality: int = Form(92),
):
    sr: Optional[SuperResService] = getattr(app.state, "sr_service", None)
    if sr is None:
        raise HTTPException(status_code=503, detail="Super-resolution disabled")

    # Manual validation (FastAPI Form() doesn't enforce ge/le)
    try:
        magnitude = int(magnitude)
    except Exception:
        raise HTTPException(status_code=400, detail="magnitude must be an integer 1..3")
    if magnitude < 1 or magnitude > 3:
        raise HTTPException(status_code=400, detail="magnitude must be 1..3")

    out_format = (out_format or "png").lower().strip()
    if out_format not in ("png", "jpeg"):
        raise HTTPException(status_code=400, detail="out_format must be png or jpeg")

    try:
        quality = int(quality)
    except Exception:
        raise HTTPException(status_code=400, detail="quality must be an integer 1..100")
    if quality < 1 or quality > 100:
        raise HTTPException(status_code=400, detail="quality must be 1..100")

    data = await file.read()
    if not data:
        raise HTTPException(status_code=400, detail="Empty upload")

    fut = sr.submit(
        data,
        out_format=out_format,
        quality=quality,
        magnitude=magnitude,
        timeout_s=0.25,
    )
    try:
        out_bytes = fut.result(timeout=SR_REQUEST_TIMEOUT)
    except Exception as e:
        msg = str(e)
        if "Queue full" in msg:
            raise HTTPException(status_code=429, detail="Too many requests (SR queue full). Try again.")
        raise HTTPException(status_code=500, detail=f"Super-resolution failed: {msg}")

    media_type = "image/jpeg" if out_format == "jpeg" else "image/png"
    return Response(
        content=out_bytes,
        media_type=media_type,
        headers={
            "Cache-Control": "no-store",
            "X-SR-Model": os.path.basename(SR_MODEL_PATH),
            "X-SR-Magnitude": str(magnitude),
            "X-SR-Passes": str(magnitude),
            "X-SR-Scale-Per-Pass": (
                str(SR_OUTPUT_SIZE // SR_INPUT_SIZE)
                if SR_OUTPUT_SIZE % SR_INPUT_SIZE == 0
                else str(SR_OUTPUT_SIZE / SR_INPUT_SIZE)
            ),
        },
    )


@app.post("/v1/superres", responses={200: {"content": {"image/png": {}, "image/jpeg": {}}}})
async def superres_v1(
    file: UploadFile = File(...),
    magnitude: int = Form(2),
    out_format: str = Form("png"),
    quality: int = Form(92),
):
    return await superres(file=file, magnitude=magnitude, out_format=out_format, quality=quality)

def _run_generate_from_dict(gen_req: dict):
    """
    Shared internal runner used by external compat endpoints.
    Returns: (bytes, seed_used, meta_headers)
    """
    # Build internal request
    req = GenerateRequest(**gen_req)

    service: PipelineService = app.state.service

    # ---- base SD generation ----
    fut = service.submit(req, timeout_s=0.25)
    png_bytes, seed = fut.result(timeout=REQUEST_TIMEOUT)

    # ---- optional SR postprocess ----
    did_superres = False
    out_bytes = png_bytes

    meta_headers = {
        "X-Seed": str(seed),
        "X-SuperRes": "0",
    }

    if req.superres:
        sr = getattr(app.state, "sr_service", None)
        if sr is None:
            # For compat callers, raise a normal exception (will become 500)
            raise RuntimeError("Super-resolution requested but SR service is disabled")

        sr_mag = int(req.superres_magnitude or 2)

        sr_fut = sr.submit(
            image_bytes=png_bytes,
            out_format=req.superres_format,
            quality=req.superres_quality,
            magnitude=sr_mag,
            timeout_s=0.25,
        )
        out_bytes = sr_fut.result(timeout=SR_REQUEST_TIMEOUT)

        did_superres = True
        meta_headers.update(
            {
                "X-SuperRes": "1",
                "X-SR-Passes": str(sr_mag),
                "X-SR-Model": os.path.basename(SR_MODEL_PATH),
                "X-SR-Scale-Per-Pass": (
                    str(SR_OUTPUT_SIZE // SR_INPUT_SIZE)
                    if SR_OUTPUT_SIZE % SR_INPUT_SIZE == 0
                    else str(SR_OUTPUT_SIZE / SR_INPUT_SIZE)
                ),
                "X-SR-Format": req.superres_format,
            }
        )

    return out_bytes, seed, meta_headers

CompatEndpoints(app=app, run_generate=_run_generate_from_dict).mount()

# UI static mount (serves Vite dist)
app.mount(
    "/",
    StaticFiles(directory="/opt/lcm-sr-server/ui-dist", html=True),
    name="ui",
)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=PORT,
        log_config=None,
    )
